{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2:  Introduction to Supervised Machine Learning with Scikit-Learn \n",
    "Welcome to the second module in the AMS Machine Learning in Python for Environmental Science Problems 2019.\n",
    "This module will introduce you to the basics of machine learning in Python.\n",
    "We'll be using scikit-learn, one of the most extensive and widely used machine learning libraries in Python.\n",
    "\n",
    "This Notebook will walk you through some examples training machine learning models.\n",
    "Some additional details can be found in separate course notes that accompany this module.\n",
    "Students are invited to follow along with these notes and retain them as an additional reference.\n",
    "\n",
    "We'll start with some preliminaries: importing the modules we'll be using and defining some functions we'll be using later. We'll revisit the functions and elaborate more on their construction later in the module as we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "import math\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.decomposition\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.tree\n",
    "import sklearn.preprocessing\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTORS = ['Forecast_Hour', 'Valid_Hour_UTC', 'Duration', 'Centroid_Lon', 'Centroid_Lat', 'Storm_Motion_U', 'Storm_Motion_V',\n",
    "         'REFL_COM_mean', 'REFL_COM_max', 'REFL_COM_min', 'REFL_COM_std', 'REFL_COM_percentile_10', 'REFL_COM_percentile_25',\n",
    "         'REFL_COM_percentile_50', 'REFL_COM_percentile_75', 'REFL_COM_percentile_90', 'U10_mean', 'U10_max', 'U10_min',\n",
    "         'U10_std', 'U10_percentile_10', 'U10_percentile_25', 'U10_percentile_50', 'U10_percentile_75', 'U10_percentile_90',\n",
    "         'V10_mean', 'V10_max', 'V10_min', 'V10_std', 'V10_percentile_10', 'V10_percentile_25', 'V10_percentile_50', 'V10_percentile_75',\n",
    "         'V10_percentile_90','T2_mean', 'T2_max', 'T2_min', 'T2_std', 'T2_percentile_10', 'T2_percentile_25', 'T2_percentile_50', \n",
    "         'T2_percentile_75', 'T2_percentile_90']\n",
    "\n",
    "#PREDICTAND = 'RVORT1_MAX-future_max'\n",
    "PREDICTAND = 'HAIL_MAXK1-future_max'\n",
    "\n",
    "SHORTEN_TRAIN = True\n",
    "SHORTEN_MAG = 250\n",
    "SAVE_FIGS = False\n",
    "\n",
    "plot_dir = './plots'\n",
    "out_dir = './models'\n",
    "\n",
    "SEPARATOR_STRING = '\\n\\n' + '*' * 50 + '\\n\\n'\n",
    "\n",
    "DEFAULT_NUM_STEPS_FOR_LOSS_DECREASE = 5\n",
    "\n",
    "MIN_DECREASE_KEY = 'min_loss_decrease'\n",
    "MIN_PERCENT_DECREASE_KEY = 'min_percentage_loss_decrease'\n",
    "NUM_STEPS_FOR_DECREASE_KEY = 'num_steps_for_loss_decrease'\n",
    "SELECTED_PREDICTORS_KEY = 'selected_predictor_name_by_step'\n",
    "LOWEST_COSTS_KEY = 'lowest_cost_by_step'\n",
    "\n",
    "CRIT_THRESH = 0.01\n",
    "CRIT_THRESH2 = 0.001\n",
    "N_FOLDS = 3\n",
    "DEFAULT_VALUE = 9999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    \"\"\"\n",
    "    Loads the data to be used for subsequent analysis.\n",
    "    \"\"\"\n",
    "    path = \"../data/track_data_ncar_ams_3km_csv_small/\"\n",
    "    #path = \"~/ams-ml-python-course/data/track_data_ncar_ams_3km_csv_small/\"\n",
    "    files = sorted(glob.glob(path+\"/*.csv\"))\n",
    "    print(\"Files:\")\n",
    "    print(files)\n",
    "    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(train_df, target, alpha=0, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    This function trains a linear model given an alpha and l1_ratio.\n",
    "    Performs unregularized linear regression by default.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : numpy array (num_recs x num_features)\n",
    "    Training matrix.\n",
    "\n",
    "    target : numpy array (num_recs)\n",
    "    Training target vector.\n",
    "\n",
    "    alpha : float\n",
    "    Regularization parameter. Nonnegative; larger value indicates more regularization.\n",
    "\n",
    "    l1_ratio : float\n",
    "    Must be between 0 and 1, inclusive; determines fraction of l1 vs l2 regularization.\n",
    "    1 is all l1; 0 is all l2. Only used materially for ElasticNet.\n",
    "    \"\"\"\n",
    "    assert 0 <= l1_ratio <= 1\n",
    "    if alpha == 0:\n",
    "        lm = sklearn.linear_model.LinearRegression()\n",
    "    elif l1_ratio == 0:\n",
    "        lm = sklearn.linear_model.Ridge(alpha=alpha)\n",
    "    elif l1_ratio == 1:\n",
    "        lm = sklearn.linear_model.Lasso(alpha=alpha)\n",
    "    else:\n",
    "        lm = sklearn.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    lm.fit(train_df, target)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(df, tar=None, n_folds=None, fold=None, test_frac=0.25):\n",
    "    \"\"\"\n",
    "    Partitions data into training and test sets and, if not already performed, into feature and target values.\n",
    "    NOTE: sklearn provides built-in functionality for this, but in general I recommend doing it yourself.\n",
    "    Data partitioning is highly important, and when dealing with correlated training examples (as is\n",
    "    typical in geoscientific problems), decisions can be quite consequential.\n",
    "    Naively letting the library perform splitting for you can inadvertently result in adverse consequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : numpy array (num_records x num_features) or pandas DataFrame\n",
    "    If tar is supplied, this is the feature matrix.\n",
    "    Otherwise, this is a DataFrame containing both feature and target values.\n",
    "    The DataFrame must have all columns in both PREDICTORS and PREDITAND.\n",
    "\n",
    "    tar : numpy array (num_records)\n",
    "    The 1-D numpy array of target values.\n",
    "\n",
    "    n_folds : int\n",
    "    Number of folds if partitioning into train and validation for cross-validation.\n",
    "    \n",
    "    fold : int\n",
    "    Current fold number (starting with 0) if partitioning into train and validation for cross-validation.\n",
    "\n",
    "    test_frac : float\n",
    "    The fraction of data to be preserved for testing/validation.\n",
    "    Only used if fold and n_folds are not supplied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_preds : numpy array (num_train_recs x num_features)\n",
    "    Training matrix.\n",
    "\n",
    "    test_preds : numpy array (num_test_recs x num_features)\n",
    "    Testing (validation) matrix.\n",
    "\n",
    "    train_tar : numpy array (num_train_recs)\n",
    "    Training target vector.\n",
    "\n",
    "    test_tar : numpy array (num_test_recs)\n",
    "    Testing (validation) target vector.\n",
    "    \"\"\"\n",
    "    if tar is None:\n",
    "        preds = df[PREDICTORS].values\n",
    "        tar = df[PREDICTAND].values\n",
    "    else:\n",
    "        preds = df\n",
    "    if n_folds is not None:\n",
    "        test_frac = 1./float(n_folds)\n",
    "    if fold is not None:\n",
    "        start_frac = fold*test_frac\n",
    "    else:\n",
    "        start_frac = 1. - test_frac\n",
    "    num_recs = preds.shape[0]\n",
    "    start_ind = int(math.floor(num_recs*start_frac))\n",
    "    end_ind = int(math.floor(num_recs*(start_frac+test_frac)))\n",
    "    train_preds = np.append(preds[:start_ind,:], preds[end_ind:,:], axis=0)\n",
    "    test_preds = preds[start_ind:end_ind,:]\n",
    "    train_tar = np.append(tar[:start_ind], tar[end_ind:], axis=0)\n",
    "    test_tar = tar[start_ind:end_ind]\n",
    "    return train_preds, test_preds, train_tar, test_tar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ss(err, err_base):\n",
    "    \"\"\"\n",
    "    Simple function to calculate skill score from a loss and reference loss.\n",
    "    \"\"\"\n",
    "    return 1. - (err / err_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sfs_on_sklearn_model(\n",
    "        training_predictor_matrix, training_target_values,\n",
    "        validation_predictor_matrix, validation_target_values, predictor_names,\n",
    "        model_object, cost_function, min_loss_decrease=None,\n",
    "        min_percentage_loss_decrease=None,\n",
    "        num_steps_for_loss_decrease=DEFAULT_NUM_STEPS_FOR_LOSS_DECREASE):\n",
    "    \"\"\"\n",
    "    @author: Ryan Lagerquist\n",
    "    Runs sequential forward selection (SFS) on scikit-learn model.\n",
    "    T = number of training examples\n",
    "    V = number of validation examples\n",
    "    P = number of predictors\n",
    "    :param training_predictor_matrix: T-by-P numpy array of predictor values.\n",
    "    :param training_target_values: length-T numpy array of target values\n",
    "        (integer class labels, since this method supports only classification).\n",
    "    :param validation_predictor_matrix: V-by-P numpy array of predictor values.\n",
    "    :param validation_target_values: length-V numpy array of target values.\n",
    "    :param predictor_names: length-P list with names of predictor variables.\n",
    "    :param model_object: Instance of scikit-learn model.  Must implement the\n",
    "        methods `fit` and `predict_proba`.\n",
    "    :param cost_function: Cost function (used to assess model on validation\n",
    "        data).  Should have the following inputs and outputs.\n",
    "    Input: target_values: Same as input `validation_target_values` for this\n",
    "        method.\n",
    "    Input: class_probability_matrix: V-by-K matrix of class probabilities, where\n",
    "        K = number of classes.  class_probability_matrix[i, k] is the predicted\n",
    "        probability that the [i]th example belongs to the [k]th class.\n",
    "    Output: cost: Scalar value.\n",
    "    :param min_loss_decrease: Used to determine stopping criterion.  If the loss\n",
    "        has decreased by less than `min_loss_decrease` over the last\n",
    "        `num_steps_for_loss_decrease` steps of sequential selection, the\n",
    "        algorithm will stop.\n",
    "    :param min_percentage_loss_decrease:\n",
    "        [used only if `min_loss_decrease is None`]\n",
    "        Used to determine stopping criterion.  If the loss has decreased by less\n",
    "        than `min_percentage_loss_decrease` over the last\n",
    "        `num_steps_for_loss_decrease` steps of sequential selection, the\n",
    "        algorithm will stop.\n",
    "    :param num_steps_for_loss_decrease: See above.\n",
    "    :return: result_dict: See documentation for `run_sfs`.\n",
    "    \"\"\"\n",
    "    num_training_examples = training_predictor_matrix.shape[0]\n",
    "    num_predictors = training_predictor_matrix.shape[1]\n",
    "    num_validation_examples = validation_predictor_matrix.shape[0]\n",
    "\n",
    "    # Create climatological model.\n",
    "    num_classes = 1 + max(\n",
    "        [numpy.max(training_target_values), numpy.max(validation_target_values)]\n",
    "    )\n",
    "\n",
    "    climo_validation_prob_matrix = numpy.full(\n",
    "        (num_validation_examples, num_classes), numpy.nan)\n",
    "    for k in range(num_classes):\n",
    "        climo_validation_prob_matrix[..., k] = numpy.mean(\n",
    "            training_target_values == k)\n",
    "\n",
    "    climo_cost = cost_function(validation_target_values,\n",
    "                               climo_validation_prob_matrix)\n",
    "\n",
    "    print('Cost of climatological model: {0:.4e}\\n'.format(climo_cost)\n",
    ")\n",
    "    # Do dirty work.\n",
    "    remaining_predictor_names = predictor_names + []\n",
    "    selected_predictor_name_by_step = []\n",
    "    lowest_cost_by_step = []\n",
    "\n",
    "    step_num = 0\n",
    "\n",
    "    while len(remaining_predictor_names) > 0:\n",
    "        print('\\n')\n",
    "        step_num += 1\n",
    "\n",
    "        lowest_cost = numpy.inf\n",
    "        best_predictor_name = None\n",
    "\n",
    "        for this_predictor_name in remaining_predictor_names:\n",
    "            #print('Trying predictor {} at step {} of SFS...').format(this_predictor_name, step_num)\n",
    "\n",
    "            these_indices = [\n",
    "                predictor_names.index(s)\n",
    "                for s in selected_predictor_name_by_step\n",
    "            ]\n",
    "            these_indices.append(predictor_names.index(this_predictor_name))\n",
    "            these_indices = numpy.array(these_indices, dtype=int)\n",
    "\n",
    "            this_training_matrix = training_predictor_matrix[..., these_indices]\n",
    "            this_validation_matrix = validation_predictor_matrix[\n",
    "                ..., these_indices]\n",
    "\n",
    "            new_model_object = sklearn.base.clone(model_object)\n",
    "            new_model_object.fit(this_training_matrix, training_target_values)\n",
    "\n",
    "            this_validation_prob_matrix = new_model_object.predict_proba(\n",
    "                this_validation_matrix)\n",
    "            this_cost = cost_function(validation_target_values,\n",
    "                                      this_validation_prob_matrix)\n",
    "\n",
    "            print('Validation loss after adding \"{}\" = {}\\n'.format(\n",
    "                this_predictor_name, this_cost))\n",
    "\n",
    "            if this_cost > lowest_cost:\n",
    "                continue\n",
    "\n",
    "            lowest_cost = this_cost + 0.\n",
    "            best_predictor_name = this_predictor_name + ''\n",
    "\n",
    "        stopping_criterion = _eval_sfs_stopping_criterion(\n",
    "            min_loss_decrease=min_loss_decrease,\n",
    "            min_percentage_loss_decrease=min_percentage_loss_decrease,\n",
    "            num_steps_for_loss_decrease=num_steps_for_loss_decrease,\n",
    "            lowest_cost_by_step=lowest_cost_by_step + [lowest_cost])\n",
    "\n",
    "        if stopping_criterion:\n",
    "            break\n",
    "\n",
    "        selected_predictor_name_by_step.append(best_predictor_name)\n",
    "        lowest_cost_by_step.append(lowest_cost)\n",
    "        remaining_predictor_names.remove(best_predictor_name)\n",
    "\n",
    "        print('Best predictor = \"{}\" ... new cost = {}'.format(\n",
    "            best_predictor_name, lowest_cost))\n",
    "        print(SEPARATOR_STRING\n",
    ")\n",
    "    return {\n",
    "        MIN_DECREASE_KEY: min_loss_decrease,\n",
    "        MIN_PERCENT_DECREASE_KEY: min_percentage_loss_decrease,\n",
    "        NUM_STEPS_FOR_DECREASE_KEY: num_steps_for_loss_decrease,\n",
    "        SELECTED_PREDICTORS_KEY: selected_predictor_name_by_step,\n",
    "        LOWEST_COSTS_KEY: lowest_cost_by_step\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_sfs_stopping_criterion(\n",
    "        min_loss_decrease, min_percentage_loss_decrease,\n",
    "        num_steps_for_loss_decrease, lowest_cost_by_step):\n",
    "    \"\"\"\n",
    "    @author: Ryan Lagerquist\n",
    "    Evaluates stopping criterion for sequential forward selection (SFS).\n",
    "    :param min_loss_decrease: If the loss has decreased by less than\n",
    "        `min_loss_decrease` over the last `num_steps_for_loss_decrease` steps,\n",
    "        the algorithm will stop.\n",
    "    :param min_percentage_loss_decrease:\n",
    "        [used only if `min_loss_decrease is None`]\n",
    "        If the loss has decreased by less than `min_percentage_loss_decrease`\n",
    "        over the last `num_steps_for_loss_decrease` steps, the algorithm will\n",
    "        stop.\n",
    "    :param num_steps_for_loss_decrease: See above.\n",
    "    :param lowest_cost_by_step: 1-D numpy array, where the [i]th value is the\n",
    "        cost after the [i]th step.  The last step is the current one, so the\n",
    "        current cost is lowest_cost_by_step[-1].\n",
    "    :return: stopping_criterion: Boolean flag.\n",
    "    :raises: ValueError: if both `min_loss_decrease` and\n",
    "        `min_percentage_loss_decrease` are None.\n",
    "    \"\"\"\n",
    "\n",
    "    if min_loss_decrease is None and min_percentage_loss_decrease is None:\n",
    "        raise ValueError('Either min_loss_decrease or '\n",
    "                         'min_percentage_loss_decrease must be specified.')\n",
    "\n",
    "    if min_loss_decrease is not None:\n",
    "        min_percentage_loss_decrease = None\n",
    "\n",
    "    if len(lowest_cost_by_step) <= num_steps_for_loss_decrease:\n",
    "        return False\n",
    "\n",
    "    previous_loss = lowest_cost_by_step[-(num_steps_for_loss_decrease + 1)]\n",
    "    if min_loss_decrease is None:\n",
    "        min_loss_decrease = previous_loss * min_percentage_loss_decrease / 100\n",
    "\n",
    "    max_new_loss = previous_loss - min_loss_decrease\n",
    "\n",
    "    #print(\n",
    "    #    'Previous loss ({0:d} steps ago) = {} ... minimum loss decrease = '\n",
    "    #    '{2:.4e} ... thus, max new loss = {} ... actual new loss = {4:.4e}'\n",
    "    #).format(num_steps_for_loss_decrease, previous_loss, min_loss_decrease,\n",
    "    #         max_new_loss, lowest_cost_by_step[-1])\n",
    "\n",
    "    return lowest_cost_by_step[-1] > max_new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression_model(model, train_df, test_df, train_tar, test_tar):\n",
    "    #Make some in- and out-of-sample predictions with the model; compare against baseline\n",
    "    train_predictions = model.predict(train_df)\n",
    "    test_predictions = model.predict(test_df)\n",
    "    baseline_predictions_train = [np.mean(train_tar)]*len(train_predictions)\n",
    "    baseline_predictions_test = [np.mean(np.append(train_tar,test_tar,axis=0))]*len(test_predictions)\n",
    "\n",
    "    # Calculate Training Error/Skill\n",
    "    mae_train = sklearn.metrics.mean_absolute_error(train_tar, train_predictions)\n",
    "    rmse_train = np.sqrt(sklearn.metrics.mean_squared_error(train_tar, train_predictions))\n",
    "    mae_train_base = sklearn.metrics.mean_absolute_error(train_tar, baseline_predictions_train)\n",
    "    rmse_train_base = np.sqrt(sklearn.metrics.mean_squared_error(train_tar, baseline_predictions_train))\n",
    "    mae_ss_train, rmse_ss_train = calc_ss(mae_train, mae_train_base), calc_ss(rmse_train, rmse_train_base)\n",
    "\n",
    "    # ...And now for Test Error/Skill\n",
    "    mae_test = sklearn.metrics.mean_absolute_error(test_tar, test_predictions)\n",
    "    rmse_test = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, test_predictions))\n",
    "    mae_test_base = sklearn.metrics.mean_absolute_error(test_tar, baseline_predictions_test)\n",
    "    rmse_test_base = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, baseline_predictions_test))\n",
    "    mae_ss_test, rmse_ss_test = calc_ss(mae_test, mae_test_base), calc_ss(rmse_test, rmse_test_base)\n",
    "\n",
    "    print('In-Sample error stats (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_train, rmse_train))\n",
    "    print('In-Sample baseline stats (MAE; RMSE): {}; {}'.format(mae_train_base, rmse_train_base))\n",
    "    print('In-Sample skill scores (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_ss_train, rmse_ss_train))\n",
    "    print('Out-of-Sample error stats (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_test, rmse_test))\n",
    "    print('Out-of-Sample baseline stats (MAE; RMSE): {}; {}'.format(mae_test_base, rmse_test_base))\n",
    "    print('Out-of-Sample skill scores (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_ss_test, rmse_ss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree_info(estimator, max_node=1000):\n",
    "    \"\"\"\n",
    "    Print some basic structural information about a supplied decision tree.\n",
    "    @author: sklearn documentation\n",
    "    Taken from: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : sklearn DecisionTree\n",
    "    The model to be analzyed.\n",
    "\n",
    "    max_node : int\n",
    "    Node number for when, if ever, to truncate printing of tree structure.\n",
    "    Optional; defaults to 1000.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # The decision estimator has an attribute called tree_  which stores the entire\n",
    "    # tree structure and allows access to low level attributes. The binary tree\n",
    "    # tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "    # array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "    # Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "    # case the values of nodes of the other type are arbitrary!\n",
    "    #\n",
    "    # Among those arrays, we have:\n",
    "    #   - left_child, id of the left child of the node\n",
    "    #   - right_child, id of the right child of the node\n",
    "    #   - feature, feature used for splitting the node\n",
    "    #   - threshold, threshold value at the node\n",
    "    #\n",
    "\n",
    "    # Using those arrays, we can parse the tree structure:\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "\n",
    "    # The tree structure can be traversed to compute various properties such\n",
    "    # as the depth of each node and whether or not it is a leaf.\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "        # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\"The binary tree structure has %s nodes and has \"\n",
    "          \"the following tree structure:\"\n",
    "          % n_nodes)\n",
    "    for i in range(n_nodes):\n",
    "        if i > max_node:\n",
    "            print(\"Exceeded max node; truncating printing\")\n",
    "            return\n",
    "        if is_leaves[i]:\n",
    "            print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "        else:\n",
    "            print(\"%snode=%s test node: go to node %s if %s <= %s else to \"\n",
    "                  \"node %s.\"\n",
    "                  % (node_depth[i] * \"\\t\",\n",
    "                     i,\n",
    "                     children_left[i],\n",
    "                     PREDICTORS[feature[i]],\n",
    "                     threshold[i],\n",
    "                     children_right[i],\n",
    "                     ))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree_info_ext(estimator, X_test):\n",
    "    \"\"\"\n",
    "    Prints additional information about a decision tree given the tree and a \n",
    "    set of validation examples.\n",
    "\n",
    "    @author: sklearn documentation (see above for link).\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : sklearn DecisionTree\n",
    "    The model to be analyzed\n",
    "   \n",
    "    X_test : numpy array (num_records x num_features)\n",
    "    Set of test or validation examples to examine.\n",
    "    \"\"\"\n",
    "    # First let's retrieve the decision path of each sample. The decision_path\n",
    "    # method allows to retrieve the node indicator functions. A non zero element of\n",
    "    # indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "    # through the node j.\n",
    "    node_indicator = estimator.decision_path(X_test)\n",
    "\n",
    "    # Similarly, we can also have the leaves ids reached by each sample.\n",
    "    leave_id = estimator.apply(X_test)\n",
    "\n",
    "    # Now, it's possible to get the tests that were used to predict a sample or\n",
    "    # a group of samples. First, let's make it for the sample.\n",
    "    sample_id = 0\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]]\n",
    "    print('Rules used to predict sample %s: ' % sample_id)\n",
    "    for node_id in node_index:\n",
    "        if leave_id[sample_id] == node_id:\n",
    "            continue\n",
    "        if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "        print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "              % (node_id,\n",
    "                 sample_id,\n",
    "                 feature[node_id],\n",
    "                 X_test[sample_id, feature[node_id]],\n",
    "                 threshold_sign,\n",
    "                 threshold[node_id]))\n",
    "    # For a group of samples, we have the following common node.\n",
    "    sample_ids = [0, 1]\n",
    "    common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                    len(sample_ids))\n",
    "    common_node_id = np.arange(n_nodes)[common_nodes]    \n",
    "    print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "          % (sample_ids, common_node_id))\n",
    "    print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_ind(inds, shape):\n",
    "    \"\"\"\n",
    "    This simple function gives the index of the flattened array given the \n",
    "    indices in an unflattened array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inds : list- or array-like\n",
    "    List of indices in the unflattened array. \n",
    "    For sensical results, must all be less than value at corresponding index in 'shape'.\n",
    "   \n",
    "    shape : list- or array-like\n",
    "    Dimension sizes in the unflattened array.\n",
    "    \"\"\"\n",
    "    ind = 0\n",
    "    for i in range(len(inds)):\n",
    "        mult = 1\n",
    "        for j in range(i+1, len(inds)):\n",
    "            mult *= shape[j]\n",
    "        ind += inds[i]*mult\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_probability_score(vals_true, vals_pred):\n",
    "    vals_true_2d = np.zeros(vals_pred.shape)\n",
    "    vals_true_2d[range(len(vals_true)), vals_true] = 1\n",
    "    fcst_cumprobs = np.cumsum(vals_pred, axis=1)\n",
    "    tv_cumprobs = np.cumsum(vals_true_2d, axis=1)\n",
    "    return np.sum((fcst_cumprobs - tv_cumprobs) ** 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we're ready to get started.\n",
    "Start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract the predictors and predictand and partition into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, train_tar, test_tar = partition_data(df)\n",
    "n_folds = N_FOLDS\n",
    "if SHORTEN_TRAIN:\n",
    "    train_df = train_df[::SHORTEN_MAG,:]\n",
    "    train_tar = train_tar[::SHORTEN_MAG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by training a simple linear regression model.\n",
    "Training a model in scikit-learn is quite straightforward.\n",
    "With all the data pre-processed, training a model is just a simple two lines of code.\n",
    "First, you call the constructor for the model.\n",
    "Second, you call the fit() function of the model, supplying your training data and target vector.\n",
    "\n",
    "A more general function that we'll be using, train_linear() -- really just a wrapper for linear regression models in sklearn -- is defined in cell 4. \n",
    "Linear regression models are in the LinearRegression class of the sklearn.linear_model module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_linear(train_df, train_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression Coefficients')\n",
    "print(lm.coef_)\n",
    "print(lm.intercept_)\n",
    "fig = plt.figure(7)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(lm.coef_)\n",
    "plt.title('Default Linear Regression Coefficients')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.xticks(range(len(PREDICTORS)))\n",
    "ax.set_xticklabels(PREDICTORS,rotation=90,fontsize=6)\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig7.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make some in- and out-of-sample predictions with the model, and compare against a simple baseline.\n",
    "For our simple baseline, we'll just have the baseline model predict the sample mean for every record.\n",
    "Making predictions with a fitted model is also a simple process, requiring just one line of code.\n",
    "All supervised learning models in scikit-learn implement, in addition to the aforementioned fit() method, a predict() method to generate predictions on new records. predict() cannot be called until fit() has already been executed for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = lm.predict(train_df)\n",
    "test_predictions = lm.predict(test_df)\n",
    "baseline_predictions_train = [np.mean(train_tar)]*len(train_predictions)\n",
    "baseline_predictions_test = [np.mean(np.append(train_tar,test_tar,axis=0))]*len(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate some error statistics for how our model performs. Two common error statistics for regression problems are Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). These are both implemented within scikit-learn in the sklearn.metrics module. By comparing with our baseline, we can also calculate skill scores.\n",
    "\n",
    "First, let's start with in-sample error/skill, also known as training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = sklearn.metrics.mean_absolute_error(train_tar, train_predictions)\n",
    "rmse_train = np.sqrt(sklearn.metrics.mean_squared_error(train_tar, train_predictions))\n",
    "mae_train_base = sklearn.metrics.mean_absolute_error(train_tar, baseline_predictions_train) \n",
    "rmse_train_base = np.sqrt(sklearn.metrics.mean_squared_error(train_tar, baseline_predictions_train))\n",
    "mae_ss_train, rmse_ss_train = calc_ss(mae_train, mae_train_base), calc_ss(rmse_train, rmse_train_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..And now for Out-of-sample/Test Error/Skill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = sklearn.metrics.mean_absolute_error(test_tar, test_predictions)\n",
    "rmse_test = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, test_predictions))\n",
    "mae_test_base = sklearn.metrics.mean_absolute_error(test_tar, baseline_predictions_test)\n",
    "rmse_test_base = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, baseline_predictions_test))\n",
    "mae_ss_test, rmse_ss_test = calc_ss(mae_test, mae_test_base), calc_ss(rmse_test, rmse_test_base)\n",
    "\n",
    "print('In-Sample error stats (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_train, rmse_train))\n",
    "print('In-Sample baseline stats (MAE; RMSE): {}; {}'.format(mae_train_base, rmse_train_base))\n",
    "print('In-Sample skill scores (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_ss_train, rmse_ss_train))\n",
    "print('Out-of-Sample error stats (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_test, rmse_test))\n",
    "print('Out-of-Sample baseline stats (MAE; RMSE): {}; {}'.format(mae_test_base, rmse_test_base))\n",
    "print('Out-of-Sample skill scores (MAE; RMSE) for Linear Regression: {}; {}'.format(mae_ss_test, rmse_ss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, perhaps unsurprisingly, that In-Sample verification paints a different (better) quantitative results than Out-of-Sample verification.\n",
    "This is a result of *overfitting*.\n",
    "Overfitting in regression models is combatted through a technique known as *regularization*.\n",
    "\n",
    "We will now look at modified (regularized) linear regression models, and see how in-sample testing is often misleading.\n",
    "    \n",
    "First, we will repeat this exercise with standardized predictors and predictand.\n",
    "Some machine learning algorithms are not *scale-invariant*; that is, they depend on the quantitative magnitude of the predictors (or predictand). When dealing with predictors with different scales or different units, this can inadvertently result in some predictors unduly influencing the model's structure compared with others. This is generally combatted by *standardizing* the predictors so that they all have the same scale and are unitless.\n",
    "\n",
    "Scikit-learn provides built-in capability for this kind of pre- (and post-) processing in the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler_tar = sklearn.preprocessing.StandardScaler()\n",
    "train_df_std = scaler.fit_transform(train_df)\n",
    "test_df_std = scaler.transform(test_df)\n",
    "train_tar_std = scaler_tar.fit_transform(np.expand_dims(train_tar, axis=1))[:,0]\n",
    "test_tar_std = scaler_tar.transform(np.expand_dims(test_tar, axis=1))[:,0]\n",
    "\n",
    "lm_std = train_linear(train_df_std, train_tar_std)    \n",
    "test_predictions_std = lm_std.predict(test_df_std)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler retains the mean/standard deviation information from the *training sample* which is used for constructing the model. It is necessary to keep it around for transforming test or validation examples in future scenarios rather than using the validation or test sample mean/std. dev. for transforming the predictors.\n",
    "\n",
    "Ultimately, we want to make our predictions in physical space, and not in the physically-insignificant standardized space. We need to transform back into physical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_std = scaler_tar.inverse_transform(test_predictions_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, the intercept should be very nearly zero, and the coefficients have the same units and are thus qualitatively comparable. The verification statistics should be identical to the non-standardized regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm_std.coef_)\n",
    "print(lm_std.intercept_)\n",
    " \n",
    "mae_test_std = sklearn.metrics.mean_absolute_error(test_tar, test_predictions_std)\n",
    "rmse_test_std = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, test_predictions_std))\n",
    "mae_ss_test_std, rmse_ss_test_std = calc_ss(mae_test_std, mae_test_base), calc_ss(rmse_test_std, rmse_test_base)\n",
    "\n",
    "print('Out-of-Sample error stats (MAE; RMSE) for Standardized Linear Regression: {}; {}'.format(mae_test_std, rmse_test_std))\n",
    "print('Out-of-Sample skill scores (MAE; RMSE) for Standardized Linear Regression: {}; {}'.format(mae_ss_test_std, rmse_ss_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to look at regularization. We'll start with Ridge Regression, which uses L2 Regularization. This penalizes the sum of the squares of weights, which in effect penalizes large weights. Consequently, the model extrapolates less from the training data supplied, and is instead drawn towards the baseline model which does not use the predictors at all (in effect has a weight vector of all zeros). The extent of this penalization can be controlled with a regularization parameter, termed 'alpha' in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_lm = train_linear(train_df_std, train_tar_std, alpha=1, l1_ratio=0)\n",
    "print(ridge_lm.coef_)\n",
    "\n",
    "ridge_predictions_train = scaler_tar.inverse_transform(ridge_lm.predict(train_df_std))\n",
    "ridge_predictions = scaler_tar.inverse_transform(ridge_lm.predict(test_df_std))\n",
    "\n",
    "mae_train_ridge = sklearn.metrics.mean_absolute_error(train_tar, ridge_predictions_train)\n",
    "rmse_train_ridge = np.sqrt(sklearn.metrics.mean_squared_error(train_tar, ridge_predictions_train))\n",
    "mae_ss_train_ridge, rmse_ss_train_ridge = calc_ss(mae_train_ridge, mae_train_base), calc_ss(rmse_train_ridge, rmse_train_base)\n",
    "\n",
    "mae_test_ridge = sklearn.metrics.mean_absolute_error(test_tar, ridge_predictions)\n",
    "rmse_test_ridge = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, ridge_predictions))\n",
    "mae_ss_test_ridge, rmse_ss_test_ridge = calc_ss(mae_test_ridge, mae_test_base), calc_ss(rmse_test_ridge, rmse_test_base)\n",
    "\n",
    "print('In-Sample error stats (MAE; RMSE) for Ridge Regression: {}; {}'.format(mae_train_ridge, rmse_train_ridge))\n",
    "print('In-Sample skill scores (MAE; RMSE) for Ridge Regression: {}; {}'.format(mae_ss_train_ridge, rmse_ss_train_ridge))\n",
    "print('Out-of-Sample error stats (MAE; RMSE) for Ridge Regression: {}; {}'.format(mae_test_ridge, rmse_test_ridge))\n",
    "print('Out-of-Sample skill scores (MAE; RMSE) for Ridge Regression: {}; {}'.format(mae_ss_test_ridge, rmse_ss_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arbitrarily picked a value of alpha, but is this best? Let's find out by testing alternate values.\n",
    "In some cases (including this one), we can have sklearn do this automatically, but let's do it manually first to see the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_alphas = np.logspace(-10, 10, 21)\n",
    "train_rmses = []; cv_rmses = []\n",
    "all_coefs = []\n",
    "best_ridge = None\n",
    "n_folds = 3\n",
    "cand_ridge_lms = []\n",
    "for ca in cand_alphas:\n",
    "    train_rmses.append(0); cv_rmses.append(0)\n",
    "    for fold in range(n_folds):\n",
    "        train_df_std_fold, val_df_std_fold, train_tar_std_fold, val_tar_std_fold = partition_data(train_df_std, train_tar_std, n_folds, fold)\n",
    "        lm_ca_ridge = train_linear(train_df_std_fold, train_tar_std_fold, alpha=ca, l1_ratio=0)\n",
    "        if fold == 0:\n",
    "            cand_ridge_lms.append(lm_ca_ridge)\n",
    "        preds = lm_ca_ridge.predict(val_df_std_fold)\n",
    "        ca_ridge_predictions_train = (lm_ca_ridge.predict(train_df_std_fold))\n",
    "        ca_ridge_predictions = (lm_ca_ridge.predict(val_df_std_fold))\n",
    "        rmse_train_ridge = np.sqrt(sklearn.metrics.mean_squared_error(train_tar_std_fold, ca_ridge_predictions_train))\n",
    "        rmse_test_ridge = np.sqrt(sklearn.metrics.mean_squared_error(val_tar_std_fold, ca_ridge_predictions))\n",
    "        train_rmses[-1] = train_rmses[-1] + rmse_train_ridge\n",
    "        cv_rmses[-1] = cv_rmses[-1] + rmse_test_ridge\n",
    "    train_rmses[-1] = train_rmses[-1] / (n_folds - 1)\n",
    "best_ind = (np.where(cv_rmses == np.min(cv_rmses))[0][0])\n",
    "best_alpha = cand_alphas[best_ind]    \n",
    "best_ridge_lm = train_linear(train_df_std, train_tar_std, alpha=best_alpha, l1_ratio=0)\n",
    "all_rmses = np.append(np.expand_dims(train_rmses,axis=0), np.expand_dims(cv_rmses,axis=0), axis=0)\n",
    "print('Coefficients with tuned regularization parameter:')\n",
    "print(best_ridge_lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the effect of alpha on the regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(111)\n",
    "all_coefs = np.zeros((len(cand_alphas[::2]),len(PREDICTORS)))\n",
    "colors = []\n",
    "for i in range(0,len(cand_alphas),2):\n",
    "    all_coefs[int(i/2),:] = cand_ridge_lms[i].coef_\n",
    "    colors.append(np.array([min(255, 40*(i/2)), min(255, 20*(i/2)), min(255,max(0, 30*((i/2)-5)))])/255.)\n",
    "    plt.plot(cand_ridge_lms[i].coef_, color=colors[-1])\n",
    "plt.title('Coefficient magnitudes vs. regularization strength')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.xticks(range(len(PREDICTORS)))\n",
    "ax.set_xticklabels(PREDICTORS,rotation=90,fontsize=6)\n",
    "plt.legend([r'$\\alpha$' + ' = %s' % alph for alph in cand_alphas[::2]], ncol=2, loc='lower left')\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig1.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the coefficients gravitate towards zero as regularization strength (alpha) increases.\n",
    "\n",
    "We can also visualize the effect of alpha on training and validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.semilogx(cand_alphas, np.transpose(all_rmses))\n",
    "print(all_rmses)\n",
    "plt.legend(['Train RMSE', 'Validation RMSE'])\n",
    "plt.xlabel('Alpha (Regularization Amount)')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.title('Error vs. Regularization Strength')\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig2.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice a number of things about this simple plot:\n",
    "\n",
    "1. Training error is consistently lower than validation/test error, though they get closer together as regularization strength increases.\n",
    "2. Training error consistently increases as regularization strength increases.\n",
    "3. In contrast, validation/test error often decreases as regularization strength increases for a time, reaches a minimum, and then increases with increasing regularization thereafter. We want to identify this regularization strength tha minimizes test error, and the steps above were conducted to identify this optimal value of alpha. This error minimization comes from a tradeoff of bias error and variance error, known as the bias/variance tradeoff, discussed in more detail in the module notes.\n",
    "4. Regularization can give you better performance than unregularized regression.\n",
    "5. In-sample error is not predictive of out-of-sample error.\n",
    "\n",
    "Sticking with basic linear regression for now, have we built the most skillful model we can, or is there still room for improvement?\n",
    "\n",
    "Let's examine an alternative kind of regularization: L1 regularization, also known as LASSO.\n",
    "\n",
    "This time, we'll look at how to do the parameter tuning automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lasso_lm = sklearn.linear_model.LassoCV(alphas=cand_alphas, cv=4)\n",
    "best_lasso_lm.fit(train_df_std, train_tar_std)\n",
    "best_alpha_lasso = best_lasso_lm.alpha_\n",
    "print('Regression coefficients for tuned LASSO:')\n",
    "print(best_lasso_lm.coef_)\n",
    "\n",
    "best_ridge_predictions = scaler_tar.inverse_transform(best_ridge_lm.predict(test_df_std))\n",
    "best_lasso_predictions = scaler_tar.inverse_transform(best_lasso_lm.predict(test_df_std))\n",
    "rmse_test_bridge = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, best_ridge_predictions))\n",
    "rmse_test_blasso = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, best_lasso_predictions))\n",
    "rmse_ss_test_bridge = calc_ss(rmse_test_bridge, rmse_test_base); rmse_ss_test_blasso = calc_ss(rmse_test_blasso, rmse_test_base)\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Unregularized Regression: {}; {}'.format(rmse_test_std, rmse_ss_test_std))\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Best Ridge Regression: {}; {}'.format(rmse_test_bridge, rmse_ss_test_bridge))\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Best LASSO Regression: {}; {}'.format(rmse_test_blasso, rmse_ss_test_blasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 regularization can also be combined in the same regression model. This is called Elastic Net.\n",
    "In Elastic Net, in addition to the overall regularization strength, the proportion of regularization in L1 vs. L2 can be controlled with an additional parameter: l1_ratio (0-1; 0 all L2, 1 all L1).\n",
    "\n",
    "Like with alpha and the LASSO, these two parameters can be automatically tuned in tandem with sklearn via grid search cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_l1_ratios = [.1, .5, .7, .9, .95, .99, 1]\n",
    "best_en_lm = sklearn.linear_model.ElasticNetCV(l1_ratio=cand_l1_ratios, alphas=cand_alphas)\n",
    "best_en_lm.fit(train_df_std, train_tar_std)\n",
    "best_en_predictions = scaler_tar.inverse_transform(best_en_lm.predict(test_df_std))\n",
    "rmse_test_ben = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, best_en_predictions))\n",
    "rmse_ss_test_ben = calc_ss(rmse_test_ben, rmse_test_base)\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Best Elastic Net Regression: {}; {}'.format(rmse_test_ben, rmse_ss_test_ben))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the regression coefficients for different kinds of regularization (or lack thereof) compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_comp = np.zeros((4, len(PREDICTORS)))\n",
    "coef_comp[0,:] = lm_std.coef_; coef_comp[1,:] = best_ridge_lm.coef_\n",
    "coef_comp[2,:] = best_lasso_lm.coef_; coef_comp[3,:] = best_en_lm.coef_\n",
    "fig = plt.figure(3)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.title('Coefficient Magnitude vs. Regularization Type Comparison')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.xticks(range(len(PREDICTORS)))\n",
    "ax.set_xticklabels(PREDICTORS,rotation=90,fontsize=6)\n",
    "[plt.plot(coef_comp[i,:]) for i in range(coef_comp.shape[0])]\n",
    "plt.ylim([-2, 2])\n",
    "plt.legend(['Unregularized', 'Ridge', 'LASSO', 'Elastic Net'])\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig3.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a way of reducing *variance* in a regression model.\n",
    "Variance reduction can also be attained in pre-processing.\n",
    "Two classes of methods that do so are feature extraction and feature selection.\n",
    "Let's take a quick look at feature extraction.\n",
    " \n",
    "Broadly speaking, feature extraction is the process of deriving new features from existing ones.\n",
    "Feature extraction is often used as a means of dimensionality reduction: finding a smaller feature set that encompasses the vast majority of the signal/variance in the training set while (hopefully) removing most of the statistical noise.\n",
    "One common and straightforward dimensionality reduction technique is Principal Component Analysis (PCA).\n",
    "Let's take a look at the process of using PCA in pre-processing for dimensionality reduction & feature extraction.\n",
    "    \n",
    "PCA is a kind of unsupervised learning; it learns structure from unlabeled training data.\n",
    "PCA is also scale-dependent, so it is paramount that the supplied features are already standardized, unless you want certain features emphasized relative to others in the dimensionality reduction phase. Typically, it's best to just leave this to the machine learning algorithm to determine.\n",
    "\n",
    "Performing dimensionality reduction often introduces one or more tunable parameters that can affect the final features that are subsequently supplied to the learning algorithm. As with the tunable parameters themselves, it is best to tune these parameters through (cross-)validation, in conjunction with any tunable parameters in the learning model.\n",
    "In the case of PCA, the one tunable parameter is the number of retained components. \n",
    "There are some heuristics that can be followed, but it is best when possible to follow normal procedure and experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA()\n",
    "\n",
    "train_df_pca = pca.fit_transform(train_df_std)\n",
    "test_df_pca = pca.transform(test_df_std)\n",
    "\n",
    "cand_frac_expl = [0.2, 0.4, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "frac_expl_by_pc = pca.explained_variance_ratio_\n",
    "cumfrac_expl_by_pc = np.cumsum(frac_expl_by_pc) \n",
    "cand_n_comps = [int(1+np.where(cumfrac_expl_by_pc >= cfe)[0][0]) for cfe in cand_frac_expl]\n",
    "print('Candidate number of components:')\n",
    "print(cand_n_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning for these parameters is not built in to any sklearn function.\n",
    "Re-implementing a full grid search is rather straightforward, but can become expensive as the training dataset size and/or number of tuned parameters increases.\n",
    "\n",
    "We will walk through an implementation of \"greedy\" tuning, which is likely but not certain to arrive at the optimal parameter configuration, and do so in far fewer experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp_ind = int(len(cand_n_comps)/2)\n",
    "l1_ratio_ind = int(len(cand_l1_ratios)/2)\n",
    "alpha_ind = int(len(cand_alphas)/2)\n",
    "    \n",
    "tune_ncomp = None; tune_alpha = None; tune_l1r = None\n",
    "n_folds = 3\n",
    "all_rmses = np.ones((len(cand_n_comps),len(cand_l1_ratios),len(cand_alphas)))*DEFAULT_VALUE\n",
    "axis = 0\n",
    "inds = [n_comp_ind, l1_ratio_ind, alpha_ind]\n",
    "bound_below = -1; bound_above = -1\n",
    "while axis < all_rmses.ndim:\n",
    "    n_comp_ind = inds[0]; l1_ratio_ind = inds[1]; alpha_ind = inds[2]\n",
    "    train_df_pca_tmp = train_df_pca[:,0:cand_n_comps[n_comp_ind]]\n",
    "    alpha = cand_alphas[alpha_ind]\n",
    "    l1_ratio = cand_l1_ratios[l1_ratio_ind]\n",
    "    model = sklearn.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    for fold in range(n_folds):\n",
    "        train_df_pca_tmp_fold, val_df_pca_tmp_fold, train_tar_std_fold, val_tar_std_fold = partition_data(train_df_pca_tmp, train_tar_std, n_folds, fold) \n",
    "        model.fit(train_df_pca_tmp_fold, train_tar_std_fold)\n",
    "        preds = model.predict(val_df_pca_tmp_fold)\n",
    "        all_rmses[n_comp_ind, l1_ratio_ind, alpha_ind] += np.sqrt(sklearn.metrics.mean_squared_error(val_tar_std_fold, preds))\n",
    "    tmp_inds = np.copy(inds)\n",
    "    tmp_inds[axis] = tmp_inds[axis]+1\n",
    "    if inds[axis] == all_rmses.shape[axis]-1:\n",
    "        bound_above = inds[axis]\n",
    "    elif np.take(all_rmses, fancy_ind(inds, all_rmses.shape)) > np.take(all_rmses, fancy_ind(tmp_inds, all_rmses.shape)) and np.take(all_rmses, fancy_ind(tmp_inds, all_rmses.shape)) < DEFAULT_VALUE:\n",
    "        bound_below = inds[axis]+1\n",
    "    tmp_inds = np.copy(inds)\n",
    "    tmp_inds[axis] = tmp_inds[axis]-1\n",
    "    if inds[axis] == 0:\n",
    "        bound_below = inds[axis]\n",
    "    elif np.take(all_rmses, fancy_ind(inds, all_rmses.shape)) > np.take(all_rmses, fancy_ind(tmp_inds, all_rmses.shape)) and  np.take(all_rmses, fancy_ind(tmp_inds, all_rmses.shape)) < DEFAULT_VALUE:\n",
    "        bound_above = inds[axis]-1\n",
    "        inds[axis] = np.where(np.take(all_rmses, range(len(all_rmses.shape[axis])), axis=axis) < DEFAULT_VALUE)[0][-1]\n",
    "    if bound_above >= 0 and bound_below >= 0:\n",
    "        n_comp_ind = np.where(all_rmses[bound_below:bound_above, l1_ratio_ind, alpha_ind] == np.min(all_rmses[bound_below:bound_above, l1_ratio_ind, alpha_ind]))[0][0] + bound_below\n",
    "        cand_rmses = all_rmses\n",
    "        for ax in range(len(inds)-1,-1,-1):\n",
    "            if ax != axis:\n",
    "                cand_rmses = np.take(cand_rmses, inds[ax], axis=ax)\n",
    "        cand_rmses = np.take(cand_rmses, range(bound_below, bound_above+1), axis=0)\n",
    "        inds[axis] = bound_below + np.where(cand_rmses == np.min(cand_rmses))[0][0]\n",
    "        axis += 1\n",
    "        bound_below = -1\n",
    "        bound_above = -1\n",
    "    elif bound_above >= 0:\n",
    "        inds[axis] = inds[axis] - 1\n",
    "    else:\n",
    "        inds[axis] = inds[axis] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a final model and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pca_tmp = train_df_pca[:,0:cand_n_comps[inds[0]]]\n",
    "alpha = cand_alphas[inds[2]]\n",
    "l1_ratio = cand_l1_ratios[inds[1]]\n",
    "pca_model = sklearn.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "pca_model.fit(train_df_pca_tmp, train_tar_std)\n",
    "best_en_pca_predictions = scaler_tar.inverse_transform(pca_model.predict(test_df_pca[:,0:cand_n_comps[inds[0]]]))\n",
    "rmse_test_bpca = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, best_en_pca_predictions))\n",
    "rmse_ss_test_bpca = calc_ss(rmse_test_bpca, rmse_test_base)\n",
    "print('Best Parameters: #PCs: {}; Alpha: {}; L1 ratio: {}'.format(cand_n_comps[inds[0]], alpha, l1_ratio))\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Best EN PCA Regression: {}; {}'.format(rmse_test_bpca, rmse_ss_test_bpca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to Classification.\n",
    "As discussed in the notes, Classification is used for categorical or discrete predictands.\n",
    "Classification models supply probabilities for each possible predictand outcome. \n",
    "Any Regression task can be converted to a Classification problem; the opposite is not true.\n",
    "Let's start simple with a binary (2-category) extension of the same prediction task we've been focusing on.\n",
    "Suppose we're interested in storms that produce non-trivial hail sizes of at least 0.01 m.\n",
    "We can discretize our predictand into examples that exceed that threshold and those that do not.\n",
    "\n",
    "We'll begin with the Classification analogy to Linear Regression for Regression which, confusingly, is known as Logistic Regression.\n",
    "\n",
    "First, we need to convert our predictand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tar_2cat = np.where(train_tar >= CRIT_THRESH, 1, 0)\n",
    "test_tar_2cat = np.where(test_tar >= CRIT_THRESH, 1, 0)\n",
    "clim_prob = (np.sum(train_tar_2cat)+np.sum(test_tar_2cat))/(len(train_tar_2cat)+len(test_tar_2cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Linear Regression, Logistic Regression can be regularized (or not).\n",
    "In sklearn currently, either L1 or L2 regularization are supported, but not both simultaneously.\n",
    "Let's stick with L2 regularization for now.\n",
    "The regularization parameter in sklearn's LogisticRegression is C, which is an *inverse* of \n",
    "regularization strength; i.e. larger numbers mean less regularization.\n",
    "There is a built-in method which will tune C via cross-validation: LogisticRegressionCV.\n",
    "We'll make use of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = sklearn.linear_model.LogisticRegressionCV(Cs=21, penalty='l2')\n",
    "log_reg.fit(train_df_std, train_tar_2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models can be interpreted in a very similar way to LinearRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Coefficients:')\n",
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)\n",
    "fig = plt.figure(8)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(log_reg.coef_)\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.xticks(range(len(PREDICTORS)))\n",
    "ax.set_xticklabels(PREDICTORS,rotation=90,fontsize=6)\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig8.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, like with Regression models, Classification algorithms have a predict() method.\n",
    "For Classification methods, this just gives the most likely (modal) verification in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_logreg_preds = log_reg.predict(test_df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally not the best way to use a classification model, nor is it typically the best way to evaluate such a model.\n",
    "\n",
    "One common and usually more robust way to verify a classification model is via the Brier Score.\n",
    "This is implemented in sklearn.\n",
    "Like with many error measures, 0 is a perfect Brier Score, and larger values are worse.\n",
    "or the Brier score, you actually use the model's event probabilities for verification, rather than just the most likely outcome.\n",
    "Probabilities can be obtained for any Classification algorithm in sklearn by using the predict_proba() method in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_logreg_preds = log_reg.predict_proba(test_df_std)\n",
    "brier_score_logreg = sklearn.metrics.brier_score_loss(test_tar_2cat, prob_logreg_preds[:,1])\n",
    "brier_score_base = sklearn.metrics.brier_score_loss(test_tar_2cat, np.ones(len(test_tar_2cat))*clim_prob)\n",
    "bss_logreg = calc_ss(brier_score_logreg, brier_score_base)\n",
    "print('Best Logistic Regression Parameters: C: {}'.format(log_reg.C_))\n",
    "print('Out-of-Sample stats (BS; BSS) for Logistic Regression Classification: {}; {}'.format(brier_score_logreg, bss_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One rather common situation in classification is to have a considerably imbalance between the event classes.\n",
    "Typically, one is interested in predicting a high-impact event (like tornado occurrence), which is often rare and then by definition, occurs much more infrequently than the non-event class.\n",
    "By default, training a model results in it seeing many more instances of the non-event class compared with the event class.\n",
    "In these situations, oftentimes developers find they get better model performance when they systematicall oversample (undersample) the event (non-event) class so that the frequency is more even. This modification can be readily performed in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_bal = sklearn.linear_model.LogisticRegressionCV(Cs=21, penalty='l2', class_weight='balanced')\n",
    "log_reg_bal.fit(train_df_std, train_tar_2cat)\n",
    "prob_logreg_bal_preds = log_reg_bal.predict_proba(test_df_std)\n",
    "brier_score_logreg_bal = sklearn.metrics.brier_score_loss(test_tar_2cat, prob_logreg_bal_preds[:,1])\n",
    "bss_logreg_bal = calc_ss(brier_score_logreg_bal, brier_score_base)\n",
    "print('Out-of-Sample stats (BS; BSS) for Balanced Logistic Regression Classification: {}; {}'.format(brier_score_logreg_bal, bss_logreg_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classification algorithms are originally designed for a binary prediction task like this one (tornado/no tornado).\n",
    "But almost all can be adapted (to varying degrees) for multi-category prediction tasks.\n",
    "\n",
    "Suppose you're still interested in the 0.1 over/under for the tornado/no tornado prediction task, but you don't feel you have enough resolution in the 'no tornado' class to have a good sense of storm impacts.\n",
    "You can alleviate this challenge while remaining in a Classification setting by simply adding a second threshold.\n",
    "In this case, suppose we consider hail over 0.001 m in size to distinguish between the existence and non-existence of hail in a storm within the \"non-significant hail\" category.\n",
    "\n",
    "The appropriate 3-category predictand can be derived with relative ease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tar_3cat = np.where(train_tar >= CRIT_THRESH, 2, np.where(train_tar >= CRIT_THRESH2, 1, 0))\n",
    "test_tar_3cat = np.where(test_tar >= CRIT_THRESH, 2, np.where(test_tar >= CRIT_THRESH2, 1, 0))\n",
    "log_reg_3cat = sklearn.linear_model.LogisticRegressionCV(Cs=21, penalty='l2', multi_class='multinomial')\n",
    "log_reg_3cat.fit(train_df_std, train_tar_3cat)\n",
    "clim_prob0 = float((len(np.where(train_tar_3cat == 0)[0]) + len(np.where(test_tar_3cat == 0)[0])))/float((len(train_tar)+len(test_tar)))\n",
    "clim_prob1 = float((len(np.where(train_tar_3cat == 1)[0]) + len(np.where(test_tar_3cat == 1)[0])))/float((len(train_tar)+len(test_tar)))\n",
    "clim_prob2 = 1. - clim_prob0 - clim_prob1\n",
    "clim_probs_3cat = np.array([clim_prob0, clim_prob1, clim_prob2])\n",
    "print('Climatological Probabilities (3-category): {}'.format(clim_probs_3cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brier score is fundamentally designed for a 2-category problem, but is readily extended to the (ordinal) multi-category case via the Rank Probability Score.\n",
    "This is not currently implemented in sklearn, so we must define it ourselves.\n",
    "This was implemented above in Cell 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_3cat_preds = log_reg_3cat.predict_proba(test_df_std)\n",
    "clim_probs_3cat_test = np.ones(log_reg_3cat_preds.shape)*clim_probs_3cat\n",
    "rps_log_reg_3cat = rank_probability_score(test_tar_3cat, log_reg_3cat_preds)\n",
    "rps_base = rank_probability_score(test_tar_3cat, clim_probs_3cat_test)\n",
    "rpss = calc_ss(rps_log_reg_3cat, rps_base)\n",
    "print('Out-of-Sample stats (RPS; RPSS) for  3-Category Logistic Regression Classification: {}; {}'.format(rps_log_reg_3cat, rpss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only looked at linear models -- those algorithms which impose a linear relationship (perhaps transformed through a non-linear link function, as in the case of Logistic Regression) between the predictors and the predictand.\n",
    "\n",
    "This is a serious limitation, though perhaps not quite as strong of an imposition as it may initially seem.\n",
    "First, the structural imposition is that there is a linear relationship between the predictand and each *predictor*, not each underlying variable. For example, if trying to predict distance traveled after 10 seconds given initial predictors of initial velocity and (known constant) acceleration, it would not be possible to back out distance traveled exactly from the raw velocity and acceleration predictors themselves using a linear model. However, by simply changing the predictors to velocity and acceleration^2, the linear assumption is valid and the distance traveled may be exactly computed.\n",
    "Second, the *predictors* are treated entirely independent, but not the underlying variables. For example, one predictor can be the product, quotient, or some complex function of two or more variables.\n",
    "Nevertheless, oftentimes we don't know exactly how the predictors relate to the predictand and/or how the predictors interrelate. If we did, the prediction task would in general be quite straightforward! \n",
    "So the linear assumption does in effect pose a major restriction on the relationships between the supplied data.\n",
    "\n",
    "Often we wish to not impose assumptions that we don't know or have particular reason to believe to be true, and would rather instead develop a more general model which does not impose such rigid assumptions.\n",
    "One example of such a model is the Decision Tree.\n",
    "The Decision Tree can be used for both Regression and Classification.\n",
    "Decision Trees are also scale-invariant, and all tree-based algorithms should not be affected by whether predictors were standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_reg_default = sklearn.tree.DecisionTreeRegressor()\n",
    "dec_tree_class_default = sklearn.tree.DecisionTreeClassifier()\n",
    "dec_tree_reg_default.fit(train_df, train_tar)\n",
    "dec_tree_class_default.fit(train_df, train_tar_2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees lack regression coefficients to easily interpret predictor-predictand relationships (although even those require care in interpretation). Instead, there are a number of ways to interpret Decision Tree models, some of which will be discussed in more detail later in the course; one of the simplest and most common approaches is through the so-called feature or variable importances.\n",
    "For now, know that each feature/predictor has an associated feature importance -- a normalized value between 0 and 1 -- where an importance of zero indicates that the feature has no impact on the prediction, and a value of 1 means it completely determines the model's prediction.\n",
    "\n",
    "If we want more detailed information about the tree structure, that can be obtained with the 'tree_' attribute of a DecisionTree. Printing of this info is implemented in Cell 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Regressor Feature Importances:')\n",
    "print(dec_tree_reg_default.feature_importances_)\n",
    "print('Decision Tree Classifier Feature Importances:')\n",
    "print(dec_tree_class_default.feature_importances_)\n",
    "print_tree_info(dec_tree_class_default)\n",
    "\n",
    "dt_reg_def_preds = dec_tree_reg_default.predict(test_df)\n",
    "dt_class_def_preds = dec_tree_class_default.predict_proba(test_df)\n",
    "brier_score_dt_def = sklearn.metrics.brier_score_loss(test_tar_2cat, dt_class_def_preds[:,1])\n",
    "bss_dt_def = calc_ss(brier_score_dt_def, brier_score_base)\n",
    "rmse_test_dt_def = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, dt_reg_def_preds))\n",
    "rmse_ss_test_dt_def = calc_ss(rmse_test_dt_def, rmse_test_base)\n",
    "print('Out-of-Sample stats (RMSE; RMSESS) for Default Decision Tree Regression: {}; {}'.format(rmse_test_dt_def, rmse_ss_test_dt_def))\n",
    "print('Out-of-Sample stats (BS; BSS) for Default Decision Tree Classification: {}; {}'.format(brier_score_dt_def, bss_dt_def))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of tunable parameters in a Decision Tree. Some concern how a split is made (e.g. 'criterion' and 'splitter', which determine how 'best' is determined and how that information is used, respectively), while most affect the termination criteria -- when to stop splitting and form a leaf; 'class_weight' also makes an appearance for a DecisionTreeClassifier. \n",
    "Fortunately, Decision Trees are less sensitive to parameter values than many other ML algorithms. Nevertheless, the values can still make a noticeable difference, and parameters should be properly tuned.\n",
    "In the case of Decision Trees, the parameters are all interrelated, so picking a couple of representative parameters is sufficient.\n",
    "The termination controlling parameters are: 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_leaf_nodes', and 'min_impurity_decrease'; you should tune at least one of these.\n",
    "'criterion' does not have much material effect on model performance in my experience.\n",
    "In some cases, 'max_features', which determines how many features to consider at each split opportunity, can impact performance.\n",
    "\n",
    "Let's start by tuning just 'min_samples_split' and 'max_features'.\n",
    "Other than the linear_model classes, most ML algorithms do not have built-in grid search cross-validation in sklearn, and it must instead be implemented manually. Last time, we implemented a manual greedy search; let's do a full grid search this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_min_samp_spls = [2, 4, 8, 16, 30, 60, 120, 250, 500, 1000]\n",
    "cand_max_feats = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "all_briers = np.zeros((len(cand_max_feats), len(cand_min_samp_spls)))\n",
    "for mf_ind in range(len(cand_max_feats)):\n",
    "    mf = cand_max_feats[mf_ind]\n",
    "    for mss_ind in range(len(cand_min_samp_spls)):\n",
    "        mss = cand_min_samp_spls[mss_ind]\n",
    "        dt = sklearn.tree.DecisionTreeClassifier(max_features=mf, min_samples_split=mss)\n",
    "        for fold in range(n_folds):\n",
    "            train_df_fold, val_df_fold, train_tar_2cat_fold, val_tar_2cat_fold = partition_data(train_df, train_tar_2cat, n_folds, fold)\n",
    "            dt.fit(train_df_fold, train_tar_2cat_fold)\n",
    "            preds = dt.predict_proba(val_df_fold)\n",
    "            all_briers[mf_ind, mss_ind] += sklearn.metrics.brier_score_loss(val_tar_2cat_fold, preds[:,1])\n",
    "mf_ind = np.where(all_briers == np.min(all_briers))[0][0]\n",
    "mss_ind = np.where(all_briers == np.min(all_briers))[1][0]\n",
    "mf = cand_max_feats[mf_ind]; mss = cand_min_samp_spls[mss_ind]\n",
    "dt = sklearn.tree.DecisionTreeClassifier(max_features=mf, min_samples_split=mss)\n",
    "dt.fit(train_df, train_tar_2cat)\n",
    "preds = dt.predict_proba(test_df)\n",
    "bs_dt_opt = sklearn.metrics.brier_score_loss(test_tar_2cat, preds[:,1])\n",
    "bss_dt_opt = calc_ss(bs_dt_opt, brier_score_base)\n",
    "print('Best Parameters: max_features: {}; min_samples_split: {}'.format(mf, mss))\n",
    "print('Out-of-Sample stats (BS; BSS) for Tuned Decision Tree Classification: {}; {}'.format(bs_dt_opt, bss_dt_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree has the advantage over linear models in that it imposes fewer (incorrect) structural assumptions (i.e. it is less *biased* than linear models), but the structure learned is highly sensitive to the particular supplied training examples (i.e. it is high *variance*), and the structure may not always extrapolate well to records unseen in the training set.\n",
    "For this reason, Decision Trees are infrequently used in isolation in modern studies and applications. \n",
    "    \n",
    "Instead, a much more popular approach is the Random Forest.\n",
    "The Random Forest uses a combination of approaches (described in course notes) to produce an ensemble many unique decision trees that can be used to make a single prediction. Technically, the steps to ensure unique decision trees result in a slight increase in *bias* in the model compared with a single decision tree in exchange for a larger decrease in model *variance*.\n",
    "\n",
    "The Random Forest can also be applied to both Regression and Classification tasks.\n",
    "As an ensemble of Decision Trees, the Random Forest shares almost all of its parameters with the DecisionTree classes.\n",
    "The main added parameter, 'n_estimators', concerns the ensemble/forest *size*.\n",
    "Unlike most parameters that have a Goldilocks value which maximizes skill, with forest size, skill tends to improve with diminishing returns with each added tree until skill plateaus, making the value a trade-off between skill and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [5, 10, 25, 50, 100, 250, 500]\n",
    "rf_bsss = np.zeros(len(n_estimators))\n",
    "times = np.zeros(len(n_estimators))\n",
    "for ne_ind in range(len(n_estimators)):\n",
    "    start = time.time()\n",
    "    ne = n_estimators[ne_ind] \n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_estimators=ne)\n",
    "    rf.fit(train_df, train_tar_2cat)\n",
    "    preds = rf.predict_proba(test_df)\n",
    "    bs_rf = sklearn.metrics.brier_score_loss(test_tar_2cat, preds[:, 1])\n",
    "    bss_rf = calc_ss(bs_rf, brier_score_base)\n",
    "    rf_bsss[ne_ind] = bss_rf\n",
    "    end = time.time()\n",
    "    times[ne_ind] = end-start\n",
    "print('Stats')\n",
    "print(\"Times (s): {}\".format(times))\n",
    "print(\"BSS: {}\".format(rf_bsss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(6)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel('Forest Size')\n",
    "ax1.set_ylabel('Brier Skill Score', color='b')\n",
    "ax2.set_ylabel('Time (s)', color='r')\n",
    "ax1.plot(n_estimators, rf_bsss, 'b')\n",
    "ax2.plot(n_estimators, times, 'r')\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig6.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to tune our RF parameters.\n",
    "For now, we'll move forward with 10 estimators. We can increase the forest size at the end to increase predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_briers_rf = np.zeros((len(cand_max_feats), len(cand_min_samp_spls)))\n",
    "for mf_ind in range(len(cand_max_feats)):\n",
    "    mf = cand_max_feats[mf_ind]\n",
    "    for mss_ind in range(len(cand_min_samp_spls)):\n",
    "        mss = cand_min_samp_spls[mss_ind]\n",
    "        rf = sklearn.ensemble.RandomForestClassifier(n_estimators=10, max_features=mf, min_samples_split=mss)\n",
    "        for fold in range(n_folds):\n",
    "            train_df_fold, val_df_fold, train_tar_2cat_fold, val_tar_2cat_fold = partition_data(train_df, train_tar_2cat, n_folds, fold)\n",
    "            rf.fit(train_df_fold, train_tar_2cat_fold)\n",
    "            preds = rf.predict_proba(val_df_fold)\n",
    "            all_briers_rf[mf_ind, mss_ind] += sklearn.metrics.brier_score_loss(val_tar_2cat_fold, preds[:,1])\n",
    "print('Step time: {}'.format(time.time() - start))\n",
    "mf_ind = np.where(all_briers_rf == np.min(all_briers_rf))[0][0]\n",
    "mss_ind = np.where(all_briers_rf == np.min(all_briers_rf))[1][0]\n",
    "mf = cand_max_feats[mf_ind]; mss = cand_min_samp_spls[mss_ind]\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_features=mf, min_samples_split=mss)\n",
    "rf.fit(train_df, train_tar_2cat)\n",
    "preds = rf.predict_proba(test_df)\n",
    "bs_rf_opt = sklearn.metrics.brier_score_loss(test_tar_2cat, preds[:, 1])\n",
    "bss_rf_opt = calc_ss(bs_rf_opt, brier_score_base)\n",
    "print('All Briers: {}'.format(all_briers_rf))\n",
    "print('Best Parameters: max_features: {}; min_samples_split: {}'.format(mf, mss))\n",
    "print('Out-of-Sample stats (BS; BSS) for Tuned Random Forest Classification: {}; {}'.format(bs_rf_opt, bss_rf_opt))\n",
    "rf_reg = sklearn.ensemble.RandomForestRegressor(n_estimators=100, max_features=mf, min_samples_split=mss)\n",
    "rf_reg.fit(train_df, train_tar)\n",
    "preds_reg = rf_reg.predict(test_df)\n",
    "rmse_test_rf_reg = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, preds_reg))\n",
    "rmse_ss_test_rf_reg = calc_ss(rmse_test_rf_reg, rmse_test_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tuned parameters, let's train and evaluate a tuned model over the whole training set with a larger forest size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_features=mf, min_samples_split=mss)\n",
    "rf.fit(train_df, train_tar_2cat)\n",
    "preds = rf.predict_proba(test_df)\n",
    "bs_rf_opt = sklearn.metrics.brier_score_loss(test_tar_2cat, preds[:, 1])\n",
    "bss_rf_opt = calc_ss(bs_rf_opt, brier_score_base)\n",
    "print('All Briers: {}'.format(all_briers_rf))\n",
    "print('Best Parameters: max_features: {}; min_samples_split: {}'.format(mf, mss))\n",
    "print('Out-of-Sample stats (BS; BSS) for Tuned Random Forest Classification: {}; {}'.format(bs_rf_opt, bss_rf_opt))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for a regression model. The optimal parameters could certainly be different; in general, they should be somewhat close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = sklearn.ensemble.RandomForestRegressor(n_estimators=100, max_features=mf, min_samples_split=mss)\n",
    "rf_reg.fit(train_df, train_tar)\n",
    "preds_reg = rf_reg.predict(test_df)\n",
    "rmse_test_rf_reg = np.sqrt(sklearn.metrics.mean_squared_error(test_tar, preds_reg))\n",
    "rmse_ss_test_rf_reg = calc_ss(rmse_test_rf_reg, rmse_test_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LinearRegression, we showed an example of applying Feature Extraction via PCA as a pre-processing step.\n",
    "As an alternative to Feature Extraction, one can also employ Feature Selection.\n",
    "\n",
    "In Feature Selection, unlike with Extraction, you preserve the native features and do not derive any new ones.\n",
    "Instead, you select a subset of the original features to use in the model, and discard the rest.\n",
    "There are numerous possible ways to implement Feature Selection.\n",
    "Here, we work through one such implementation, known as Sequential Forward Selection (SFS).\n",
    "\n",
    "In SFS, you sequentially greedily build up your feature set by adding the feature which reduces validation error the most, until adding new features no longer improves performance by enough to warrant the added feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_exp = sklearn.ensemble.RandomForestClassifier(n_estimators=5, max_features=mf, min_samples_split=mss)\n",
    "sfs_results = run_sfs_on_sklearn_model(train_df_fold, train_tar_2cat_fold, val_df_fold, val_tar_2cat_fold, PREDICTORS, rf_exp, rank_probability_score, min_percentage_loss_decrease=0.01)\n",
    "print('Sequential Forward Selection Results:')\n",
    "print(sfs_results['selected_predictor_name_by_step'])\n",
    "inds = [PREDICTORS.index(pred) for pred in sfs_results['selected_predictor_name_by_step']]\n",
    "train_df_select = np.take(train_df, inds, axis=1)\n",
    "test_df_select = np.take(test_df, inds, axis=1)\n",
    "rf_select = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_features=mf, min_samples_split=mss)\n",
    "rf_select.fit(train_df_select, train_tar_2cat)\n",
    "preds = rf_select.predict_proba(test_df_select)\n",
    "bs_rf_select = sklearn.metrics.brier_score_loss(test_tar_2cat, preds[:, 1])\n",
    "bss_rf_select = calc_ss(bs_rf_select, brier_score_base)\n",
    "print('Out-of-Sample stats (BS; BSS) for Random Forest Classification w/ Feature Selection: {}; {}'.format(bs_rf_select, bss_rf_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's close out by looking at the results of some different implementations alongside one another.\n",
    "Start with Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rsmess = [rmse_ss_test, rmse_ss_test_std, rmse_ss_test_ridge, rmse_ss_test_bridge, rmse_ss_test_blasso, rmse_ss_test_ben, rmse_ss_test_bpca, rmse_ss_test_dt_def, rmse_ss_test_rf_reg]\n",
    "reg_legend = ['Linear Regression', 'Standardized Linear Regression', 'Default Ridge Regression', 'Tuned Ridge Regression', 'LASSO Regression', 'Elastic Net Regresion', 'PCA Regression', 'Decision Tree', 'Random Forest']\n",
    "plt.figure(4)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(range(len(reg_legend)), all_rsmess, tick_label=reg_legend)\n",
    "plt.ylabel('Root Mean Squared Error Skill Score')\n",
    "plt.xlabel('Model')\n",
    "plt.title('Regression Model Skill Comparison')\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig4.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "plt.xticks(rotation='vertical')\n",
    "all_bss = [bss_logreg, bss_logreg_bal, rpss, bss_dt_def, bss_dt_opt, bss_rf_opt, bss_rf_select]    \n",
    "class_legend = ['Logistic Regression', 'Balanced Logistic Regression', '3-Category Logistic Regression', 'Default Decision Tree', 'Tuned Decision Tree', 'Random Forest', 'Random Forest w/ Feat Selection']\n",
    "plt.bar(range(len(class_legend)), all_bss, tick_label=class_legend)\n",
    "plt.ylabel('Brier Skill Score')\n",
    "plt.xlabel('Model')\n",
    "plt.title('Classification Model Skill Comparison')\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(plot_dir + '/' + 'short_course_fig5.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save our RandomForestClassifier so that we can use it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf, out_dir + '/' + 'tutorial_rf_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other machine learning algorithms for both regression and classification tasks. See my course notes for a brief description of some of them, and stay tuned for more detailed discussion of one class of algorithms after lunch: (deep) neural nets!\n",
    "\n",
    "These cutting edge algorithms present some benefits (as well as a few drawbacks) compared with those we have already discussed, and have a promising future in development.\n",
    "\n",
    "But most importantly, sklearn makes it easy to try out different approaches. So when computational/time resources allow, try out different methods and discover which one(s) work best for your research task. Sometimes the result will surprise you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
