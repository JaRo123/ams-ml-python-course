{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1:  Data Analysis and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to short course data and problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the problem is to predict the probability of low-level vorticity exceeding a certain threshold up to ahead given a storm with a simulated radar reflectivity > 40 dBZ and the associated surface wind and temperature fields. \n",
    "\n",
    "__Input fields in the netCDF data:__\n",
    "\n",
    "* REFL_COM_curr (composite reflectivity)\n",
    "\n",
    "* U10_curr (10 m west-east wind component in m/s)\n",
    "\n",
    "* V10_curr (10 m south-north wind component in m/s)\n",
    "\n",
    "* T2_curr (2 m temperature in Kelvin)\n",
    "\n",
    "__Prediction field:__\n",
    "\n",
    "* RVORT1_MAX_future (hourly maximum vertical vorticity at 1 km Above ground level in s-1)\n",
    "\n",
    "__Other fields of note:__\n",
    "\n",
    "* time: valid time of storm image\n",
    "\n",
    "* i and j: row and column array indices from original WRF model grid\n",
    "\n",
    "* x and y: projection coordinates in m\n",
    "\n",
    "* masks: binary grid showing where storm outline is. Aggregate stats in csv file are extracted from only positive grid points in masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading meteorological data files with pandas and xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libraries used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to find the CSV files and create a sorted list of the found files\n",
    "To do this, we use the glob library to list all of the *.csv files in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the data as a string\n",
    "path = \"../../../data/track_data_ncar_ams_3km_csv_small/\"\n",
    "\n",
    "# create a list of the files and print it out\n",
    "files = sorted(glob.glob(path+\"/*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to read in all of the found CSV files using Pandas and concat all of the content\n",
    "This method adds the content of all of the csv files into one Python Pandas DataFrame object.  We also print the data's column labels in order to help us determine which keys we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "print (df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the CSV data\n",
    "It's always important to understand your data before you do anything with it!  If you don't understand your data, your analysis will be difficult and conclusions can be incorrect.\n",
    "\n",
    "\n",
    "##### First lets get a subsection of the data into a DataFrame object by using the labels found by printing the list of keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:,['Step_ID', 'U10_mean', 'V10_mean', 'T2_mean']]\n",
    "print (type(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once we have the data within a DataFrame, it's easy to get the mean by issueing this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['T2_mean'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's also easy to create a quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['T2_mean'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NetCDF files\n",
    "\n",
    "##### How to find the NetCDF files and create a sorted list of the found files\n",
    "To do this, we use the glob library to list all of the *.nc files in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the data as a string\n",
    "path = \"../../../data/track_data_ncar_ams_3km_nc_small/\"\n",
    "\n",
    "# create a list of the files and print it out\n",
    "files = sorted(glob.glob(path+\"/*.nc\"))\n",
    "#print (files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Let's read in the first file in the list and see what is in the files.\n",
    " \n",
    " In the below cell, we open the first file within the file list and print out its summary information.  Afterwards, we close the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Open the file with the Netcdf library for reading and then close it\n",
    "nf = nc.Dataset(files[0], \"r\")\n",
    "print (nf)\n",
    "nf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets open a file with xarray and print its content\n",
    "\n",
    "As you probably have noticed, the output format is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = xr.open_dataset(files[0])\n",
    "print (xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are a few different ways to look at the different parts of a NetCDF file using xarray\n",
    "\n",
    "You can reference variables as keys within the xarray object.\n",
    "\n",
    "You can use the variable's attributes to reference the dimensions, coordinates, and attributes of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = xf['T2_curr']\n",
    "print (T)\n",
    "print (\"Dimensions:\",T.dims)\n",
    "print (\"Coords:\",T.coords)\n",
    "print (\"Attributes:\",T.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing and Selecting Data Ranges\n",
    "\n",
    "There are difference ways to retrieve the values of a variable. \n",
    "\n",
    "You can use indexing similar to numpy arrays.  You can also index using diminsion names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (T[0,0,1].values)\n",
    "print (T[0:5,0,1].values)\n",
    "print (T.sel(p=slice(0,5),col=1,row=0).values)\n",
    "print (T.sel(p=4,col=1,row=0).values)\n",
    "print (T.sel(row=0,col=1,p=4).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "For these examples, we will use the xarray object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an array where the values are larger than 290 and values less than 290 are added as nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T.where(T>290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an array where values greater than 290 are True and values less than 290 are False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T>290"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Round all values to the nearest integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the mean of all the values for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.mean(dim=['col','row','p']).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the min of all values for that variable and then find the min across all columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (T.min())\n",
    "print (T.min(dim=['col','row']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the max of all values for that variable and then find the min across all columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (T.max())\n",
    "print (T.max(dim=['col','row']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the 5th percentile of the data along the 'p' axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (T.groupby('p').reduce(np.percentile, q=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory visulization with matplotlib \n",
    "As with anything you do with Python, there are multiple ways of doing the same thing.  Here are a couple of ways to create plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to create a simple line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[:,1,1].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to create a simple plot of p=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.isel(p=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to create a timeseries plot over two locations using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xf['p'],T[:,1,1], label='Location 1')\n",
    "plt.plot(xf['p'],T[:,30,30], label='Location 2')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to create a simple contour plot with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.contour(T[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Create the same plot as above, but use the axis label provided by xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(T.sel(p=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same as above, but countour labels have been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(T.sel(p=0))\n",
    "plt.clabel(cs, fmt='%.0f', inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set some variables that are used in the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = xf['V10_curr']\n",
    "U = xf['U10_curr']\n",
    "r = xf['row']\n",
    "c = xf['col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to draw a countour plot with quivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(U.sel(p=0))\n",
    "plt.clabel(cs, fmt='%.0f', inline=True)\n",
    "plt.quiver(r, c, U.sel(p=0), V.sel(p=0), pivot='middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to draw a barb plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.barbs(r, c, U.sel(p=200), V.sel(p=200), length=5, pivot='middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for the turorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Declare all of the input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = []\n",
    "valid_times = []\n",
    "# List of input variables\n",
    "in_vars = [\"REFL_COM_curr\",\n",
    "           \"U10_curr\", \"V10_curr\"]\n",
    "# List of output variables\n",
    "out_vars = [\"RVORT1_MAX_future\"]\n",
    "in_data = []\n",
    "out_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop through the first 5 files and extract the relevant variables\n",
    "We're only operating on a couple of files for the following example to save on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files[0:5]:\n",
    "    run_time = pd.Timestamp(f.split(\"/\")[-1].split(\"_\")[1])\n",
    "    ds = xr.open_dataset(f)\n",
    "    in_data.append(np.stack([ds[v].values for v in in_vars], axis=-1))\n",
    "    out_data.append(np.stack([ds[v].values for v in out_vars], axis=-1))\n",
    "    valid_times.append(ds[\"time\"].values)\n",
    "    run_times.append([run_time] * in_data[-1].shape[0])\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stack the  data into single arrays instead of lists of arrays\n",
    "This is done to make it easier to feed the data into the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_data = np.vstack(in_data)\n",
    "all_out_data = np.vstack(out_data)\n",
    "all_run_times = np.concatenate(run_times)\n",
    "all_valid_times = np.concatenate(valid_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deallocate the lists of temporary arrays to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del in_data[:], out_data[:], run_times[:], valid_times[:]\n",
    "del in_data, out_data, run_times, valid_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the maximum vorticity values in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vort = all_out_data[:, :, :, 0].max(axis=-1).max(axis=-1)\n",
    "vort_thresh = 0.008\n",
    "print(percentileofscore(max_vort, vort_thresh))\n",
    "vort_labels = np.where(max_vort > vort_thresh, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create some histograms that show the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(max_vort, bins=50, cumulative=True, density=True)\n",
    "plt.plot(np.ones(10) * vort_thresh, np.linspace(0, 1, 10))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid()\n",
    "plt.xlabel(\"1 km AGL Relative Vorticity\")\n",
    "plt.ylabel(\"Cumulative Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(all_in_data.shape[-1], 1, figsize=(6, all_in_data.shape[-1] * 4))\n",
    "for a, ax in enumerate(axes):\n",
    "    ax.hist(all_in_data[:, :, :, a].ravel(), 50)\n",
    "    ax.set_ylabel(in_vars[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a storm example using what we've gone over so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_ex = max_vort.argmax()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pcolormesh(all_in_data[rot_ex, :, :, 0], cmap=\"gist_ncar\", vmin=-10, vmax=85)\n",
    "plt.colorbar()\n",
    "plt.quiver(all_in_data[rot_ex, :, :, 1], all_in_data[rot_ex, :, :, 2])\n",
    "plt.contour(all_out_data[rot_ex, :, :, 0])\n",
    "plt.title(\"Storm Example {0} Valid \".format(rot_ex) + pd.Timestamp(all_valid_times[rot_ex]).strftime(\"%Y-%m-%d %H:%M\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating into training and test sets \n",
    "We need to separate the full data set into two different groups.  The first group is what we feed into the model to train it.  The second group is what we use to test with to see if the model performs as expected.  It's important to create the groups correctly by knowing your data.  For example, is your data time dependant? If so, would randomly assigning data to these groups make it harder for the model to pick up on patterns?  \n",
    "\n",
    "Picking the correct amount of data to put in each group is equally as important.  Picking the incorrect amount of data (and also picking the incorrect groups) can cause overfitting.  This happens when the model that is generated isn't generalized enough for prediction.\n",
    "\n",
    "You can try different combinations to see how it effects your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pd.Timestamp(\"2010-10-28\")\n",
    "train_indices = np.where(all_run_times < split_date)[0]\n",
    "test_indices = np.where(all_run_times >= split_date)[0]\n",
    "print (\"Size of training set: \",len(train_indices))\n",
    "print (\"Size of test set: \",len(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By random index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "percent_train = .8\n",
    "\n",
    "indices = range(len(U.coords['p']))\n",
    "shuffle(indices)\n",
    "\n",
    "split = int(len(U.coords['p'])*.8)\n",
    "print (\"Splitting on index: \",split)\n",
    "\n",
    "train_indices = indices[0:split]\n",
    "test_indices = indices[split:len(U.coords['p'])-1]\n",
    "\n",
    "print (\"Size of training set: \",len(train_indices))\n",
    "print (\"Size of test set: \",len(test_indices))\n",
    "\n",
    "#print (train_indices)\n",
    "#print (test_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train = .8\n",
    "\n",
    "split = int(len(U.coords['p'])*.8)\n",
    "print (\"Splitting on index: \",split)\n",
    "\n",
    "train_indices = np.array(range(0,split))\n",
    "test_indices = np.array(range(split, len(U.coords['p'])))\n",
    "print (\"Size of training set: \",len(train_indices))\n",
    "print (\"Size of test set: \",len(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing patch data\n",
    "Normalizing the data allows observational data to be more easily predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "U = xf['U10_curr']\n",
    "\n",
    "U = U.stack(z=('row','col'))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(U)\n",
    "ua_norm = scaler.transform(U)\n",
    "\n",
    "ua_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Using prinicpal component analysis to reduce the dimensionality of the different fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pc_objs = []\n",
    "means = []\n",
    "sds = []\n",
    "\n",
    "num_comps = 1\n",
    "num_vars = ua_norm.shape[0]\n",
    "pc_train_data = np.zeros((train_indices.size, ua_norm.shape[1]), dtype=np.float32)\n",
    "pc_test_data = np.zeros((test_indices.size, ua_norm.shape[1]), dtype=np.float32)\n",
    "for v in range(num_vars):\n",
    "    pc_objs.append(PCA(n_components=num_comps))\n",
    "    var_data = ua_norm\n",
    "    pc_train_data[:, v * num_comps: (v + 1) * num_comps] = pc_objs[v].fit_transform(var_data[train_indices])\n",
    "    pc_test_data[:, v * num_comps: (v + 1) * num_comps] = pc_objs[v].transform(var_data[test_indices])\n",
    "    del var_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
